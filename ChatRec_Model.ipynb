{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3677f812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9249c5f1dc40d68fdbad957417571b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbced9b6ce1841eb9d0972585adf7126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "  Train Loss: 13.1087\n",
      "  Val Loss: 12.7720 | Perplexity: 352230.92\n",
      "  -> Saved best model checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1664e47a15946bda0ff3c9e4ddb7073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbc618dd8e84c91ac180823e6740de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3\n",
      "  Train Loss: 13.1682\n",
      "  Val Loss: 12.5049 | Perplexity: 269654.85\n",
      "  -> Saved best model checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8498b080d2f64bbeb8568c377efed06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d3d944426d4a97b20c2c8bb3263ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3\n",
      "  Train Loss: 12.9028\n",
      "  Val Loss: 12.1080 | Perplexity: 181310.87\n",
      "  -> Saved best model checkpoint.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA_FILE = \"conversationfile.xlsx - userAuserB.csv\"\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "MAX_CONTEXT_TURNS = 5 \n",
    "MAX_LENGTH = 256\n",
    "SEED = 42\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4 \n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    'user_a': '<USER_A>',\n",
    "    'user_b': '<USER_B>',\n",
    "    'reply_start': '<REPLY>', \n",
    "    'bos_token': '<BOS>',\n",
    "    'eos_token': '<EOS>',\n",
    "    'pad_token': '<PAD>'\n",
    "}\n",
    "TOKEN_LIST = list(SPECIAL_TOKENS.values())\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df = df.sort_values(by=['Conversation ID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "def format_conversation_for_gpt2(df, max_context_turns):\n",
    "    conversations = []\n",
    "    \n",
    "    for conv_id, conv_df in df.groupby('Conversation ID'):\n",
    "        conv_list = conv_df.to_dict('records')\n",
    "        \n",
    "        for i in range(1, len(conv_list)):\n",
    "            current_msg = conv_list[i]\n",
    "            prev_msg = conv_list[i-1]\n",
    "            \n",
    "            if current_msg['Sender'] == 'User A' and prev_msg['Sender'] == 'User B':\n",
    "                \n",
    "                history_start_idx = max(0, i - max_context_turns - 1)\n",
    "                context_history = conv_list[history_start_idx : i-1]\n",
    "                \n",
    "                context_parts = [\n",
    "                    f\"{SPECIAL_TOKENS['user_a'] if msg['Sender'] == 'User A' else SPECIAL_TOKENS['user_b']} {msg['Message']}\"\n",
    "                    for msg in context_history\n",
    "                ]\n",
    "                \n",
    "                full_text = (\n",
    "                    f\"{SPECIAL_TOKENS['bos_token']} \"\n",
    "                    f\"{' '.join(context_parts).strip()} \" \n",
    "                    f\"{SPECIAL_TOKENS['user_b']} {prev_msg['Message']} \"\n",
    "                    f\"{SPECIAL_TOKENS['reply_start']} \" \n",
    "                    f\"{current_msg['Message']} \" \n",
    "                    f\"{SPECIAL_TOKENS['eos_token']}\"\n",
    "                )\n",
    "                \n",
    "                conversations.append(full_text)\n",
    "            \n",
    "    return conversations\n",
    "\n",
    "formatted_conversations = format_conversation_for_gpt2(df, MAX_CONTEXT_TURNS)\n",
    "train_texts, test_texts = train_test_split(formatted_conversations, test_size=0.1, random_state=SEED)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [SPECIAL_TOKENS['user_a'], SPECIAL_TOKENS['user_b'], SPECIAL_TOKENS['reply_start']],\n",
    "    'bos_token': SPECIAL_TOKENS['bos_token'],\n",
    "    'eos_token': SPECIAL_TOKENS['eos_token'],\n",
    "    'pad_token': SPECIAL_TOKENS['pad_token']\n",
    "})\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_texts)\n",
    "test_encodings = tokenize_texts(test_texts)\n",
    "\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attention_mask = encodings['attention_mask']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.input_ids[idx].clone() \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "train_dataset = ConversationDataset(train_encodings)\n",
    "test_dataset = ConversationDataset(test_encodings)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_loss(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "    val_loss, perplexity = evaluate_loss(model, test_dataloader, DEVICE)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Perplexity: {perplexity:.2f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt') \n",
    "        print(\"  -> Saved best model checkpoint.\")\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt', map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "def generate_reply(model, tokenizer, last_user_b_message, context_history_list, max_new_tokens=50):\n",
    "    \n",
    "    model.eval()\n",
    "    history_string = ' '.join(context_history_list)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"{SPECIAL_TOKENS['bos_token']} \"\n",
    "        f\"{history_string.strip()} \"\n",
    "        f\"{SPECIAL_TOKENS['user_b']} {last_user_b_message} \"\n",
    "        f\"{SPECIAL_TOKENS['reply_start']}\" \n",
    "    )\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt', max_length=MAX_LENGTH, truncation=True).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    try:\n",
    "        reply_start_index = generated_text.find(SPECIAL_TOKENS['reply_start']) + len(SPECIAL_TOKENS['reply_start'])\n",
    "        reply_end_index = generated_text.find(SPECIAL_TOKENS['eos_token'], reply_start_index)\n",
    "        \n",
    "        reply = generated_text[reply_start_index:reply_end_index].strip()\n",
    "        \n",
    "        for token in TOKEN_LIST:\n",
    "            reply = reply.replace(token, '').strip()\n",
    "            \n",
    "        reply_parts = re.split(r'[.?!]', reply.strip())\n",
    "        return reply_parts[0].strip()\n",
    "        \n",
    "    except:\n",
    "        return \"Generation Error: Could not parse reply.\"\n",
    "\n",
    "\n",
    "def extract_context_and_reference(formatted_text):\n",
    "    try:\n",
    "        reply_start = formatted_text.find(SPECIAL_TOKENS['reply_start']) + len(SPECIAL_TOKENS['reply_start'])\n",
    "        reply_end = formatted_text.find(SPECIAL_TOKENS['eos_token'])\n",
    "        reference = formatted_text[reply_start:reply_end].strip()\n",
    "\n",
    "        prompt_text = formatted_text[:formatted_text.find(SPECIAL_TOKENS['reply_start'])].strip()\n",
    "        prompt_text = prompt_text.replace(SPECIAL_TOKENS['bos_token'], '').strip()\n",
    "        \n",
    "        last_b_idx = prompt_text.rfind(SPECIAL_TOKENS['user_b'])\n",
    "        \n",
    "        last_user_b_message = prompt_text[last_b_idx + len(SPECIAL_TOKENS['user_b']):].strip()\n",
    "        context_history_string = prompt_text[:last_b_idx].strip()\n",
    "        \n",
    "        history_list = [\n",
    "            turn.strip() for turn in context_history_string.split() \n",
    "            if turn.strip() and turn.strip() not in TOKEN_LIST\n",
    "        ]\n",
    "\n",
    "        return last_user_b_message, context_history_string.split(), reference\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def evaluate_generation(model, tokenizer, test_texts):\n",
    "    \n",
    "    generated_replies = []\n",
    "    reference_replies = []\n",
    "    \n",
    "    for text in test_texts[:50]:\n",
    "        last_b_msg, history_list, reference = extract_context_and_reference(text)\n",
    "        \n",
    "        if last_b_msg and reference and reference.strip():\n",
    "            generated = generate_reply(model, tokenizer, last_b_msg, history_list)\n",
    "            \n",
    "            generated_replies.append(generated)\n",
    "            reference_replies.append(reference)\n",
    "\n",
    "    smooth_function = SmoothingFunction().method4\n",
    "    bleu_scores = [\n",
    "        sentence_bleu([ref.split()], gen.split(), smoothing_function=smooth_function)\n",
    "        for ref, gen in zip(reference_replies, generated_replies)\n",
    "    ]\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l_scores = [\n",
    "        scorer.score(ref, gen)['rougeL'].fmeasure\n",
    "        for ref, gen in zip(reference_replies, generated_replies)\n",
    "    ]\n",
    "    avg_rouge_l = np.mean(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "    \n",
    "    final_loss, perplexity = evaluate_loss(model, test_dataloader, DEVICE)\n",
    "\n",
    "    print(f\"--- Final Evaluation Results ---\")\n",
    "    print(f\"Number of samples evaluated: {len(generated_replies)}\")\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1: {avg_rouge_l:.4f}\")\n",
    "    print(f\"Final Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'avg_bleu': avg_bleu,\n",
    "        'avg_rouge_l': avg_rouge_l,\n",
    "        'perplexity': perplexity\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
